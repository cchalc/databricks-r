# Point to the OSS package path, e.g., /path/to/.../spark-2.4.0-bin-hadoop2.7
library(SparkR, lib.loc = .libPaths(c(file.path('/Users/christopher.chalcraft/Databricks/dbconnect/.env/lib/python3.8/site-packages/pyspark', 'R', 'lib'), .libPaths())))
# Point to the Databricks Connect PySpark installation, e.g., /path/to/.../pyspark
Sys.setenv(SPARK_HOME = "/Users/christopher.chalcraft/Databricks/dbconnect/.env/lib/python3.8/site-packages/pyspark")
# Point to the OSS package path, e.g., /path/to/.../spark-2.4.0-bin-hadoop2.7
library(SparkR, lib.loc = .libPaths(c(file.path('/Users/christopher.chalcraft/Programs/spark-3.1.1-bin-hadoop2.7', 'R', 'lib'), .libPaths())))
# Point to the Databricks Connect PySpark installation, e.g., /path/to/.../pyspark
Sys.setenv(SPARK_HOME = "/Users/christopher.chalcraft/Databricks/dbconnect/.env/lib/python3.8/site-packages/pyspark")
# Point to the OSS package path, e.g., /path/to/.../spark-2.4.0-bin-hadoop2.7
library(SparkR, lib.loc = .libPaths(c(file.path('/Users/christopher.chalcraft/Programs/spark-3.1.1-bin-hadoop2.7', 'R', 'lib'), .libPaths())))
# Point to the Databricks Connect PySpark installation, e.g., /path/to/.../pyspark
Sys.setenv(SPARK_HOME = "/Users/christopher.chalcraft/Databricks/dbconnect/.env/lib/python3.8/site-packages/pyspark")
# Point to the OSS package path, e.g., /path/to/.../spark-2.4.0-bin-hadoop2.7
library(SparkR, lib.loc = .libPaths(c(file.path('/Users/christopher.chalcraft/Programs/spark-3.1.1-bin-hadoop2.7', 'R', 'lib'), .libPaths())))
# Point to the Databricks Connect PySpark installation, e.g., /path/to/.../pyspark
Sys.setenv(SPARK_HOME = "/Users/christopher.chalcraft/Databricks/dbconnect/.env/lib/python3.8/site-packages/pyspark")
install.packages("sparklyr")
sparkR.session()
library(SparkR, lib.loc = .libPaths(c(file.path('/Users/christopher.chalcraft/Programs/spark-3.1.1-bin-hadoop2.7', 'R', 'lib'), .libPaths())))
Sys.setenv(SPARK_HOME = "/Users/christopher.chalcraft/Databricks/dbconnect/.env/lib/python3.8/site-packages/pyspark")
sparkR.session()
df <- as.DataFrame(faithful)
# Point to the OSS package path, e.g., /path/to/.../spark-2.4.0-bin-hadoop2.7
library(SparkR, lib.loc = .libPaths(c(file.path('/Users/christopher.chalcraft/Programs/spark-3.1.1-bin-hadoop2.7', 'R', 'lib'), .libPaths())))
# Point to the Databricks Connect PySpark installation, e.g., /path/to/.../pyspark
Sys.setenv(SPARK_HOME = "/Users/christopher.chalcraft/Databricks/dbconnect/.env/lib/python3.8/site-packages/pyspark")
sparkR.session()
df <- as.DataFrame(faithful)
head(df)
df1 <- dapply(df, function(x) { x }, schema(df))
collect(df1)
setwd("~/Databricks/databricks-r")
install.packages(c("odbc", "DBI"), repos = "https://cran.microsoft.com/snapshot/2022-02-24/")
library(odbc)
library(DBI)
install.packages(c("odbc", "DBI"), repos = "https://cran.microsoft.com/snapshot/2022-02-24/")
install.packages(c("odbc", "DBI"), repos = "https://cran.microsoft.com/snapshot/2022-02-24/")
print(dbGetQuery(conn, "SELECT * FROM christopherchalcraft_scratch.bike_sharing LIMIT 2"))
library(DBI)
dbGetQuery(conn, "SELECT * FROM christopherchalcraft_scratch.bike_sharing LIMIT 2")
install.packages(c("odbc", "DBI"), repos = "https://cran.microsoft.com/snapshot/2022-02-24/")
library(odbc)
library(DBI)
conn = dbConnect(
drv = odbc(),
dsn = "Databricks"
)
install.packages(c("odbc", "DBI"), repos = "https://cran.microsoft.com/snapshot/2022-02-24/")
library(odbc)
library(DBI)
conn = dbConnect(
drv = odbc(),
dsn = "Databricks"
)
library(odbc)
library(DBI)
conn = dbConnect(
drv = odbc(),
dsn = "Databricks"
)
odbc::odbcListDrivers()
